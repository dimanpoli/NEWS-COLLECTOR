"""
–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–π—Ç–∞ RIA.ru (—Ä–∞–∑–¥–µ–ª —ç–∫–æ–Ω–æ–º–∏–∫–∏)
"""

import time
import logging
import random
from datetime import datetime
from typing import List, Optional, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup as BS
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from parsers.base_parser import BaseParser
from models.news_item import NewsItem

logger = logging.getLogger(__name__)


class RiaParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å RIA.ru"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ RIA"""
        super().__init__(source_name="ria")
        self.base_url = "https://ria.ru/economy/"
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º HTTP-—Å–µ—Å—Å–∏—é —Å —Ä–µ—Ç—Ä–∞—è–º–∏ –∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
        self.session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–µ—Ç—Ä–∞–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
        retry_strategy = Retry(
            total=3,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            backoff_factor=1,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            status_forcelist=[429, 500, 502, 503, 504],  # –ö–æ–¥—ã –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–µ—Ç—Ä–∞—è
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)
        
        # –ë–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Referer': 'https://www.google.com/',
        })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫—É–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.session.cookies.update({
            'acceptCookies': 'true',
            'cookieConsent': '1',
        })
    
    def parse_last_hour(self) -> List[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å RIA.ru –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
        """
        try:
            logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å RIA.ru")
            
            # –ü–æ–ª—É—á–∞–µ–º HTML –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            html_content = self._fetch_page_with_retry(self.base_url)
            if not html_content:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É RIA.ru")
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
            news_links = self._extract_news_links(html_content)
            logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(news_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏")
            
            if not news_links:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏")
                return []
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
            news_items = []
            max_news_to_parse = 10  # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ 10 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ä–∞–∑
            
            for i, link in enumerate(news_links[:max_news_to_parse]):
                try:
                    logger.debug(f"üì∞ –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–∏ {i+1}/{min(len(news_links), max_news_to_parse)}: {link}")
                    
                    news_item = self._parse_news_page(link)
                    if news_item:
                        news_items.append(news_item)
                        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {news_item.title[:60]}...")
                    
                    # –°–ª—É—á–∞–π–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (1-3 —Å–µ–∫—É–Ω–¥—ã)
                    time.sleep(random.uniform(1.0, 3.0))
                    
                except Exception as e:
                    logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–∏ {link}: {e}")
                    continue
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
            filtered_items = self.filter_by_time(news_items, hours_back=1)
            
            logger.info(f"üìä –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(filtered_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å")
            return filtered_items
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RIA: {e}")
            return []
    
    def _fetch_page_with_retry(self, url: str, max_retries: int = 3) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(max_retries):
            try:
                # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
                time.sleep(random.uniform(0.5, 1.5))
                
                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                response = self.session.get(
                    url, 
                    timeout=(10, 30),  # 10 —Å–µ–∫ –Ω–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ, 30 —Å–µ–∫ –Ω–∞ —á—Ç–µ–Ω–∏–µ
                    allow_redirects=True,
                    verify=True  # –í–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL
                )
                
                response.raise_for_status()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ HTML, –∞ –Ω–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∫–∞–ø—á–µ–π
                if any(x in response.text.lower() for x in ['captcha', '—Ä–æ–±–æ—Ç', 'bot', '–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω']):
                    logger.warning(f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∞–ø—á—É: {url}")
                    if attempt < max_retries - 1:
                        time.sleep(5)  # –ñ–¥–µ–º –ø–æ–¥–æ–ª—å—à–µ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                        continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                response.encoding = response.apparent_encoding or 'utf-8'
                
                return response.text
                
            except requests.exceptions.SSLError as e:
                logger.warning(f"‚ö†Ô∏è SSL –æ—à–∏–±–∫–∞ –¥–ª—è {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    continue
                    
            except requests.exceptions.Timeout as e:
                logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –¥–ª—è {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    time.sleep(3)
                    continue
                    
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
                    
            except requests.exceptions.HTTPError as e:
                logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –¥–ª—è {url}: {e}")
                return None
                
            except Exception as e:
                logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {url}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
        
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        return None
    
    def _extract_news_links(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ HTML"""
        soup = BS(html, 'html.parser')
        links = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏–∫–∏
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è RIA.ru
        selectors = [
            "a[href*='/economy/']",  # –°—Å—ã–ª–∫–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ —ç–∫–æ–Ω–æ–º–∏–∫–∏
            "a[href*='/20'][href*='.html']",  # –°—Å—ã–ª–∫–∏ —Å –¥–∞—Ç–∞–º–∏
            "div.list-item__content a.list-item__title",  # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            "a.cell-list__item-link[href*='/20']",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
            "article a[href*='/20']",  # –ù–æ–≤–æ—Å—Ç–∏ –≤ —Å—Ç–∞—Ç—å—è—Ö
        ]
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                for elem in elements:
                    href = elem.get('href', '')
                    if href and href.startswith('https://ria.ru/'):
                        # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        href = href.split('#')[0].split('?')[0]
                        if href not in links:
                            links.append(href)
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {e}")
                continue
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        unique_links = list(set(links))
        return unique_links[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 20 —Å—Å—ã–ª–∫–∞–º–∏
    
    def _parse_news_page(self, url: str) -> Optional[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–¥–µ–ª—å–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        """
        try:
            html = self._fetch_page_with_retry(url)
            if not html:
                return None
            
            soup = BS(html, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(soup)
            if not title:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è {url}")
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_at = self._extract_date(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
            text = self._extract_text(soup)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
            if not text or len(text.strip()) < 100:
                logger.debug(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è {url}")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏
            news_item = NewsItem(
                source=self.source_name,
                link=url,
                title=title,
                text=text,
                published_at=published_at
            )
            
            return news_item
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None
    
    def _extract_title(self, soup: BS) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_selectors = [
            "h1.article__title",
            "h1.m-article__title",
            "meta[property='og:title']",
            "meta[name='title']",
            "title",
        ]
        
        for selector in title_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    if selector.startswith("meta"):
                        title = element.get('content', '').strip()
                    else:
                        title = element.get_text(strip=True)
                    
                    if title and len(title) > 5:
                        return title
            except:
                continue
        
        return ""
    
    def _extract_date(self, soup: BS) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –¥–∞—Ç—ã
        date_selectors = [
            "meta[property='article:published_time']",
            "meta[name='published_time']",
            "div.article__info-date a",
            "time.article__date",
            "div.article__date",
            "meta[itemprop='datePublished']",
        ]
        
        date_str = ""
        for selector in date_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    if selector.startswith("meta"):
                        date_str = element.get('content', '')
                    else:
                        date_str = element.get_text(strip=True)
                    
                    if date_str:
                        break
            except:
                continue
        
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏
        return self._parse_date_string(date_str)
    
    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Å –¥–∞—Ç–æ–π –≤ –æ–±—ä–µ–∫—Ç datetime"""
        if not date_str:
            return None
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        date_str = date_str.replace('T', ' ').replace('Z', '').strip()
        
        # –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç –Ω–∞ RIA
        date_formats = [
            "%Y-%m-%d %H:%M:%S",        # 2024-01-15 14:30:00
            "%Y-%m-%d %H:%M",           # 2024-01-15 14:30
            "%d.%m.%Y %H:%M",           # 15.01.2024 14:30
            "%Y-%m-%dT%H:%M:%S%z",      # 2024-01-15T14:30:00+0300
            "%H:%M %d.%m.%Y",           # 14:30 15.01.2024
            "%d %B %Y, %H:%M",          # 15 —è–Ω–≤–∞—Ä—è 2024, 14:30
        ]
        
        for date_format in date_formats:
            try:
                return datetime.strptime(date_str, date_format)
            except ValueError:
                continue
        
        logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str}")
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –º–∏–Ω—É—Å 1 —á–∞—Å
        return datetime.now()
    
    def _extract_text(self, soup: BS) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏"""
        # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ RIA
        text_selectors = [
            "div.article__text",
            "div.article__body",
            "div.article-content",
            "article",
            "div[itemprop='articleBody']",
        ]
        
        for selector in text_selectors:
            try:
                article_body = soup.select_one(selector)
                if article_body:
                    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—Ä–µ–∫–ª–∞–º–∞, —Å—Å—ã–ª–∫–∏ –∏ —Ç.–¥.)
                    for unwanted in article_body.select("script, style, iframe, .banner, .ad, .social"):
                        unwanted.decompose()
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                    paragraphs = article_body.find_all(['p', 'h2', 'h3', 'h4'])
                    texts = []
                    
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                            texts.append(text)
                    
                    if texts:
                        return '\n'.join(texts)
            except:
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å —Å–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            all_paragraphs = soup.find_all('p')
            texts = [p.get_text(strip=True) for p in all_paragraphs 
                    if len(p.get_text(strip=True)) > 50]
            
            if texts:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 15 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                return '\n'.join(texts[:15])
        except:
            pass
        
        return ""